{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fasfaisa/emotion_detection/blob/main/face_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "o_dHm8xGelyM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "# Deep Learning Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sgRmqilXlBy0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2554612a-68d2-4e52-b845-72404b8c515c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def load_images_from_folder(folder_path):\n",
        "    \"\"\"\n",
        "    Load images from a folder with subdirectories representing emotions\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the main folder containing emotion subfolders\n",
        "\n",
        "    Returns:\n",
        "        tuple: (images, labels, emotion_map)\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    emotion_map = {}\n",
        "\n",
        "    # Define emotions based on subdirectory names\n",
        "    emotions = sorted(os.listdir(folder_path))\n",
        "\n",
        "    # Create emotion to index mapping\n",
        "    for idx, emotion in enumerate(emotions):\n",
        "        emotion_map[emotion] = idx\n",
        "\n",
        "        # Path to specific emotion folder\n",
        "        emotion_path = os.path.join(folder_path, emotion)\n",
        "\n",
        "        # Skip if not a directory\n",
        "        if not os.path.isdir(emotion_path):\n",
        "            continue\n",
        "\n",
        "        # Load images for this emotion\n",
        "        for filename in os.listdir(emotion_path):\n",
        "            img_path = os.path.join(emotion_path, filename)\n",
        "\n",
        "            try:\n",
        "                # Read image\n",
        "                img = load_img(img_path, target_size=(48, 48), color_mode='grayscale')\n",
        "\n",
        "                # Convert to numpy array\n",
        "                img_array = img_to_array(img)\n",
        "\n",
        "                # Normalize\n",
        "                img_array /= 255.0\n",
        "\n",
        "                images.append(img_array)\n",
        "                labels.append(idx)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {img_path}: {e}\")\n",
        "\n",
        "    return (np.array(images), np.array(labels), emotion_map)\n",
        "\n",
        "# Set paths\n",
        "zip_path = '/content/drive/MyDrive/archive.zip'\n",
        "extract_path = '/content/drive/MyDrive/FER2013/extracted'\n",
        "\n",
        "# Load images\n",
        "X, y, emotion_labels = load_images_from_folder(os.path.join(extract_path, 'train'))\n",
        "\n",
        "# Reshape for CNN\n",
        "X = X.reshape(-1, 48, 48, 1)\n",
        "\n",
        "# One-hot encode labels\n",
        "y = to_categorical(y, num_classes=len(emotion_labels))\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "luNq0kZ3lTQ7"
      },
      "outputs": [],
      "source": [
        "# Step 4:\n",
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tDvU3WWWlqQy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Advanced CNN Model\n",
        "def create_emotion_model(input_shape=(48, 48, 1), num_classes=len(emotion_labels)):\n",
        "    model = Sequential([\n",
        "        # First Convolutional Block\n",
        "        Conv2D(64, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Second Convolutional Block\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Third Convolutional Block\n",
        "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.25),\n",
        "\n",
        "        # Flatten and Dense Layers\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OjLIKv12mDV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9134e95a-0124-45a3-cea8-4eea0b549e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create and compile the model\n",
        "model = create_emotion_model()\n",
        "\n",
        "# Ensure .keras extension for model checkpoint\n",
        "checkpoint_path = '/content/drive/MyDrive/best_emotion_model.keras'\n",
        "\n",
        "# Callbacks\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    checkpoint_path,\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max'\n",
        ")\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    verbose=1,\n",
        "    min_lr=0.00001\n",
        ")\n",
        "\n",
        "early_stopper = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAbGt9vbmINX",
        "outputId": "e4254bac-d5fe-4e6e-e88d-b2cedfa300b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m 10/288\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22:15\u001b[0m 5s/step - accuracy: 0.1555 - loss: 3.1330"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 7: Train the Model\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=64, subset='training'),\n",
        "    validation_data=datagen.flow(X_train, y_train, subset='validation'),\n",
        "    epochs=50,\n",
        "    callbacks=[model_checkpoint, lr_reducer, early_stopper]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa1aB68lmJr8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 8: Model Evaluation\n",
        "# Load the best saved model\n",
        "best_model = load_model(checkpoint_path)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true_classes, y_pred_classes, target_names=emotion_labels))\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=emotion_labels,\n",
        "            yticklabels=emotion_labels)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwewfL9vmLPv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 9: Training History Visualization\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot Training Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Training Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlELjx_jmM1K"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 10: Real-time Emotion Detection Function\n",
        "def detect_emotion(frame, face_cascade, emotion_model):\n",
        "    # Convert frame to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Load pre-trained Haar cascade for face detection\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    # Detect faces\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Extract face ROI\n",
        "        roi_gray = gray[y:y+h, x:x+w]\n",
        "\n",
        "        # Resize to 48x48\n",
        "        roi_gray = cv2.resize(roi_gray, (48, 48))\n",
        "\n",
        "        # Normalize\n",
        "        roi_gray = roi_gray / 255.0\n",
        "\n",
        "        # Reshape for model input\n",
        "        roi_gray = roi_gray.reshape(1, 48, 48, 1)\n",
        "\n",
        "        # Predict emotion\n",
        "        prediction = emotion_model.predict(roi_gray)\n",
        "        emotion_index = np.argmax(prediction)\n",
        "\n",
        "        # Draw rectangle and label\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, emotion_labels[emotion_index], (x, y-10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
        "\n",
        "    return frame\n",
        "\n",
        "# Save the model for future use\n",
        "model.save('/content/drive/MyDrive/FER2013/emotion_recognition_model.h5')\n",
        "\n",
        "print(\"Emotion Recognition Model Training Completed!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDBffCZEGGZrtO3Qr1BLaZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}